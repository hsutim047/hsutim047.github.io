---
---

@inproceedings{wu2022efficacy,
  abbr={PMLR},
  field={Speech},
  field_badge_class={badge_speech},
  title={The efficacy of self-supervised speech models for audio representations},
  author={Hsu*, Tsu-Yuan and Wu*, Tung-Yu and Li*, Chen-An and Lin*, Tzu-Han and Lee, Hung-yi},
  booktitle={HEAR: Holistic Evaluation of Audio Representations},
  pages={90--110},
  year={2022},
  organization={PMLR},
  paper="https://proceedings.mlr.press/v166/wu22a",
  code="https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge---NTU-GURA",
}

@inproceedings{huang2023improving,
  abbr={SLT},
  field={Speech},
  field_badge_class={badge_speech},
  title={Improving generalizability of distilled self-supervised speech processing models under distorted settings},
  author={Hsu*, Tsu-Yuan and Huang*, Kuan-Po and Fu*, Yu-Kuan and Gutierrez, Fabian Ritter and Wang, Fan-Lin and Tseng, Liang-Hsuan and Zhang, Yu and Lee, Hung-yi},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={1112--1119},
  year={2023},
  organization={IEEE},
  paper="https://ieeexplore.ieee.org/abstract/document/10022474",
  code="https://github.com/nobel861017/distort-robust-distilSSL",
}

@inproceedings{huang2023ensemble,
  abbr={ICASSP},
  field={Speech},
  field_badge_class={badge_speech},
  title={Ensemble knowledge distillation of self-supervised speech models},
  author={Huang*, Kuan-Po and Feng*, Tzu-hsun and Fu, Yu-Kuan and Hsu, Tsu-Yuan and Yen, Po-Chieh and Tseng, Wei-Cheng and Chang, Kai-Wei and Lee, Hung-yi},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE},
  paper="https://ieeexplore.ieee.org/abstract/document/10096445",
}

@inproceedings{hsu2023visually,
  abbr={ACL},
  field={Multimodal},
  field_badge_class={badge_multimodal},
  title={Visually-Enhanced Phrase Understanding},
  author={Hsu*, Tsu-Yuan and Li*, Chen-An and Huang, Chao-Wei and Chen, Yun-Nung},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={5879--5888},
  year={2023},
  paper="https://aclanthology.org/2023.findings-acl.363/",
  code="https://github.com/MiuLab/VisualLU",
}

@inproceedings{huang-etal-2023-converser,
  abbr={SIGDIAL},
  field={Text},
  field_badge_class={badge_text},
  title = "{CONVERSER}: Few-shot Conversational Dense Retrieval with Synthetic Data Generation",
  author = "Huang, Chao-Wei  and
    Hsu, Chen-Yu  and
    Hsu, Tsu-Yuan  and
    Li, Chen-An  and
    Chen, Yun-Nung",
  editor = "Schlangen, David  and
    Stoyanchev, Svetlana  and
    Joty, Shafiq  and
    Dusek, Ondrej  and
    Kennington, Casey  and
    Alikhani, Malihe",
  booktitle = "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue",
  month = sep,
  year = "2023",
  address = "Prague, Czechia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.sigdial-1.34",
  pages = "381--387",
  abstract = "Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose Converser, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed Converser achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and generated datasets are available: https://github.com/MiuLab/CONVERSER",
  paper="https://aclanthology.org/2023.sigdial-1.34",
  code="https://github.com/MiuLab/CONVERSER"
}

@article{kuan2023towards,
  abbr={ASRU},
  field={Multimodal},
  field_badge_class={badge_multimodal},
  title={Towards General-Purpose Text-Instruction-Guided Voice Conversion},
  author={Kuan, Chun-Yi and Li, Chen An and Hsu, Tsu-Yuan and Lin, Tse-Yang and Chung, Ho-Lam and Chang, Kai-Wei and Chang, Shuo-yiin and Lee, Hung-yi},
  journal={arXiv preprint arXiv:2309.14324},
  year={2023},
  paper="https://arxiv.org/abs/2309.14324",
}