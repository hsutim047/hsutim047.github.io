---
---

@inproceedings{wu2022efficacy,
  abbr={PMLR},
  field={Speech},
  title={The efficacy of self-supervised speech models for audio representations},
  author={Wu, Tung-Yu and Hsu, Tsu-Yuan and Li, Chen-An and Lin, Tzu-Han and Lee, Hung-yi},
  booktitle={HEAR: Holistic Evaluation of Audio Representations},
  pages={90--110},
  year={2022},
  organization={PMLR}
}

@inproceedings{huang2023improving,
  abbr={SLT},
  field={Speech},
  title={Improving generalizability of distilled self-supervised speech processing models under distorted settings},
  author={Huang, Kuan-Po and Fu, Yu-Kuan and Hsu, Tsu-Yuan and Gutierrez, Fabian Ritter and Wang, Fan-Lin and Tseng, Liang-Hsuan and Zhang, Yu and Lee, Hung-yi},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={1112--1119},
  year={2023},
  organization={IEEE}
}

@inproceedings{huang2023ensemble,
  abbr={ICASSP},
  field={Speech},
  title={Ensemble knowledge distillation of self-supervised speech models},
  author={Huang, Kuan-Po and Feng, Tzu-hsun and Fu, Yu-Kuan and Hsu, Tsu-Yuan and Yen, Po-Chieh and Tseng, Wei-Cheng and Chang, Kai-Wei and Lee, Hung-yi},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{hsu2023visually,
  abbr={ACL},
  field={Multimodal},
  title={Visually-Enhanced Phrase Understanding},
  author={Hsu, Tsu-Yuan and Li, Chen-An and Huang, Chao-Wei and Chen, Yun-Nung},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={5879--5888},
  year={2023}
}

@inproceedings{huang-etal-2023-converser,
  abbr={SIGDIAL},
  field={Text},
  title = "{CONVERSER}: Few-shot Conversational Dense Retrieval with Synthetic Data Generation",
  author = "Huang, Chao-Wei  and
    Hsu, Chen-Yu  and
    Hsu, Tsu-Yuan  and
    Li, Chen-An  and
    Chen, Yun-Nung",
  editor = "Schlangen, David  and
    Stoyanchev, Svetlana  and
    Joty, Shafiq  and
    Dusek, Ondrej  and
    Kennington, Casey  and
    Alikhani, Malihe",
  booktitle = "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue",
  month = sep,
  year = "2023",
  address = "Prague, Czechia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.sigdial-1.34",
  pages = "381--387",
  abstract = "Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose Converser, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed Converser achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and generated datasets are available: https://github.com/MiuLab/CONVERSER",
}